{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abdbfc6-1d3e-4c75-b304-f636f3984ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2bded-1e7b-415b-bf07-a564c1c96d9f",
   "metadata": {},
   "source": [
    "# Load Instruction Fine Tuning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e399c787-521f-4334-857e-615c68d5db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./llava_instruct_150k.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a28181-1e6f-4b5b-b0c6-10be6ab19d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class LLaVADataset(Dataset):\n",
    "    def __init__(self, json_path, image_dir, max_length=142, limit=None):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        # Limit the dataset to the specified number of samples\n",
    "        if limit is not None:\n",
    "            self.data = self.data[10000:10000+limit]\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.max_length = max_length\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((336, 336)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['image'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        # Combine all conversation texts into a single string\n",
    "        conversation_text = \"\"\n",
    "        for conv in item['conversations']:\n",
    "            if conv['from'] == 'human':\n",
    "                conversation_text += f\"Human: {conv['value']}\\n\"\n",
    "            elif conv['from'] == 'gpt':\n",
    "                conversation_text += f\"Assistant: {conv['value']}\\n\"\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'conversation_text': conversation_text\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "json_path = './llava_instruct_150k.json'\n",
    "image_dir = './coco/train2017/'\n",
    "tiny_dataset = LLaVADataset(json_path, image_dir, limit=5000)\n",
    "dataloader = DataLoader(tiny_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46046967-39eb-4ff1-bf95-4cc42db8223c",
   "metadata": {},
   "source": [
    "# Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc4a252-dd9f-44fb-8231-dee31f59cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bbca0022644d30a7887688d64d6a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "llava_tokenizer, llava_model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path),\n",
    "    load_in_8bit=True\n",
    ")\n",
    "device = \"cuda\"\n",
    "llava_model.model = llava_model.model.to(device)\n",
    "llava_model.model = llava_model.model.to(torch.float16)\n",
    "\n",
    "for param in llava_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5934e9-7162-4fd3-83ec-42d1a9fd3bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to ./exported_llava_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenizer to a directory\n",
    "save_path = \"./exported_llava_tokenizer\"\n",
    "llava_tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45c1cfe-0d3d-4124-bd1e-6a19040eb810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenizer_config.json', 'special_tokens_map.json', 'tokenizer.model']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2208e3-993a-482f-981a-770ad1f0e335",
   "metadata": {},
   "source": [
    "# Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd08ec0e-38cd-45be-8421-4e7668739e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLLAVA(nn.Module):\n",
    "    def __init__(self, vision_encoder, projection_head, text_decoder, tokenizer, max_seq_length=4096, device=\"cuda\"):\n",
    "        super(TinyLLAVA, self).__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.projection_head = projection_head\n",
    "        self.text_decoder = text_decoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.vision_encoder.to(device)\n",
    "        self.projection_head.to(device)\n",
    "        self.text_decoder.to(device)\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Extract visual features\n",
    "        with torch.no_grad():\n",
    "            visual_features = self.vision_encoder(image)  # Shape: (batch_size, vision_feature_dim)\n",
    "    \n",
    "        # Project visual features to text embedding space\n",
    "        projected_features = self.projection_head(visual_features).to(self.device)  # Move to the same device\n",
    "    \n",
    "        # Embed input tokens\n",
    "        token_embeddings = self.text_decoder.transformer.wte(input_ids).to(self.device)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "    \n",
    "        # Combine visual features with token embeddings\n",
    "        combined_embeddings = torch.cat(\n",
    "            [projected_features.unsqueeze(1), token_embeddings], dim=1\n",
    "        ).to(self.device)\n",
    "    \n",
    "        # Adjust attention mask to include visual tokens\n",
    "        _ones = torch.ones((attention_mask.size(0), 1)).to(self.device)\n",
    "        extended_attention_mask = torch.cat(\n",
    "            [_ones, attention_mask], dim=1\n",
    "        ).to(self.device)\n",
    "    \n",
    "        # Truncate combined embeddings and attention mask to max_seq_length if needed\n",
    "        # if combined_embeddings.size(1) > self.max_seq_length:\n",
    "        #     # print(\"T\")\n",
    "        #     combined_embeddings = combined_embeddings[:, :self.max_seq_length]\n",
    "        #     extended_attention_mask = extended_attention_mask[:, :self.max_seq_length]\n",
    "    \n",
    "        # Forward pass through the text decoder\n",
    "        outputs = self.text_decoder(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=extended_attention_mask\n",
    "        )\n",
    "        outputs.logits = outputs.logits[:, 1:, :]  # Get rid of distillgpt input token\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e271f3-00db-4352-a103-88c2dae59c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Encoder Ready\n",
      "Updated max_position_embeddings: 4096\n",
      "Positional embeddings shape: torch.Size([4096, 768])\n",
      "LLM and Tokenizer Ready\n",
      "Projection Head Ready\n"
     ]
    }
   ],
   "source": [
    "vision_encoder = models.mobilenet_v3_small()\n",
    "vision_encoder.classifier[-1] = torch.nn.Linear(vision_encoder.classifier[-1].in_features, 768)\n",
    "\n",
    "vision_encoder.load_state_dict(torch.load('./mobilenetv3_student_model.pth', map_location=torch.device('cpu'), weights_only=True))\n",
    "vision_encoder.eval()\n",
    "\n",
    "for param in vision_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Vision Encoder Ready\")\n",
    "\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "llm.lm_head = nn.Linear(in_features=768, out_features=32000) # llava out features\n",
    "\n",
    "# Set the new max position embeddings\n",
    "llm.config.max_position_embeddings = 4096  # Update the max position embeddings\n",
    "\n",
    "# Resize positional embeddings (wpe) to match new max_position_embeddings\n",
    "old_embeddings = llm.transformer.wpe.weight.data  # Original embeddings\n",
    "new_seq_length = llm.config.max_position_embeddings  # Desired sequence length\n",
    "\n",
    "# Interpolate to resize\n",
    "new_embeddings = torch.nn.functional.interpolate(\n",
    "    old_embeddings.unsqueeze(0).transpose(1, 2),  # Add batch dimension for interpolation\n",
    "    size=new_seq_length,  # New sequence length\n",
    "    mode=\"linear\",\n",
    "    align_corners=False,\n",
    ").squeeze(0).transpose(1, 0)  # Remove batch dimension and revert dimensions\n",
    "\n",
    "# Update the embeddings in the model\n",
    "llm.transformer.wpe.weight.data = new_embeddings\n",
    "\n",
    "# Verify changes\n",
    "print(f\"Updated max_position_embeddings: {llm.config.max_position_embeddings}\")\n",
    "print(f\"Positional embeddings shape: {llm.transformer.wpe.weight.shape}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "print(\"LLM and Tokenizer Ready\")\n",
    "\n",
    "projection_head = nn.Linear(768, 768).to(device)\n",
    "print(\"Projection Head Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a53876-a9ea-4757-8f82-76a1a803814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_llava = TinyLLAVA(vision_encoder, projection_head, llm, llava_tokenizer).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfeb131c-7c18-4647-9713-113052041d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'conversation_text'])\n"
     ]
    }
   ],
   "source": [
    "for x in dataloader:\n",
    "    print(x.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535dbc26-af9a-46f5-9018-f0ababbf3fb0",
   "metadata": {},
   "source": [
    "# Performing Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84d58d99-ac91-4049-aa41-f7b12a075d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import log_softmax, softmax, cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(tiny_llava, llava_model, dataloader, optimizer, temperature=1.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Perform one epoch of knowledge distillation training.\n",
    "\n",
    "    Args:\n",
    "        tiny_llava: The student model.\n",
    "        llava_model: The teacher model (frozen).\n",
    "        dataloader: DataLoader for the training data.\n",
    "        optimizer: Optimizer for the student model.\n",
    "        temperature: Temperature for knowledge distillation.\n",
    "\n",
    "    Returns:\n",
    "        Average loss for the epoch.\n",
    "    \"\"\"\n",
    "    tiny_llava.train()\n",
    "    kl_div_loss = KLDivLoss(reduction=\"batchmean\")\n",
    "    total_loss = 0.0\n",
    "    total_cosine_similarity = 0.0\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training Batch\", leave=True)\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Load batch data\n",
    "        images = batch[\"image\"].to(device)\n",
    "        conversation_texts = batch[\"conversation_text\"]\n",
    "\n",
    "        # Tokenize conversation text\n",
    "        inputs = tiny_llava.tokenizer(conversation_texts, return_tensors=\"pt\", padding=True, truncation=True).to(tiny_llava.device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        batch_size = input_ids.shape[0]\n",
    "        # Teacher model forward pass\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = llava_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images.to(torch.float16)\n",
    "            ).logits\n",
    "\n",
    "        # Student model forward pass\n",
    "        student_outputs = tiny_llava(image=images, input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        # Compute KD loss\n",
    "        student_log_probs = log_softmax(student_outputs / temperature, dim=-1)\n",
    "        teacher_probs = softmax(teacher_outputs / temperature, dim=-1)\n",
    "        loss = kl_div_loss(student_log_probs, teacher_probs) * (temperature ** 2)\n",
    "    \n",
    "        # print(student_outputs.view(-1).shape, student_outputs.view(-1).shape)\n",
    "        # Compute cosine similarity\n",
    "        cosine_sim = cosine_similarity(\n",
    "            student_outputs.reshape(batch_size,-1), teacher_outputs.reshape(batch_size,-1), dim=1\n",
    "        ).mean().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        total_cosine_similarity += cosine_sim\n",
    "\n",
    "        # Update progress bar\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        avg_cosine_similarity = total_cosine_similarity / (batch_idx + 1)\n",
    "        progress_bar.set_postfix({\n",
    "            \"Avg Loss\": f\"{avg_loss:.4f}\",\n",
    "            \"Avg Cosine Sim\": f\"{avg_cosine_similarity:.4f}\"\n",
    "        })\n",
    "\n",
    "    return total_loss / len(dataloader), total_cosine_similarity/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07c6eb5-f46c-4aa2-be4d-fa509eb771b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(tiny_llava, llava_model, dataloader, optimizer, temperature=1.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Perform one epoch of knowledge distillation training.\n",
    "\n",
    "    Args:\n",
    "        tiny_llava: The student model.\n",
    "        llava_model: The teacher model (frozen).\n",
    "        dataloader: DataLoader for the training data.\n",
    "        optimizer: Optimizer for the student model.\n",
    "        temperature: Temperature for knowledge distillation.\n",
    "\n",
    "    Returns:\n",
    "        Average loss for the epoch.\n",
    "    \"\"\"\n",
    "    tiny_llava.train()\n",
    "    mse_loss_fn = MSELoss()\n",
    "    total_mse_loss = 0.0\n",
    "    total_cosine_similarity = 0.0\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training Batch\", leave=True)\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Load batch data\n",
    "        images = batch[\"image\"].to(device)\n",
    "        conversation_texts = batch[\"conversation_text\"]\n",
    "\n",
    "        # Tokenize conversation text\n",
    "        inputs = tiny_llava.tokenizer(conversation_texts, return_tensors=\"pt\", padding=True, truncation=True).to(tiny_llava.device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        batch_size = input_ids.shape[0]\n",
    "        # Teacher model forward pass\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = llava_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images.to(torch.float16)\n",
    "            ).logits\n",
    "\n",
    "        # Student model forward pass\n",
    "        student_outputs = tiny_llava(image=images, input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        # Compute MSE loss\n",
    "        mse_loss = mse_loss_fn(student_outputs, teacher_outputs)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cosine_sim = cosine_similarity(\n",
    "            student_outputs.reshape(batch_size, -1), teacher_outputs.reshape(batch_size, -1), dim=1\n",
    "        ).mean().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics\n",
    "        total_mse_loss += mse_loss.item()\n",
    "        total_cosine_similarity += cosine_sim\n",
    "\n",
    "        # Update progress bar\n",
    "        avg_mse_loss = total_mse_loss / (batch_idx + 1)\n",
    "        avg_cosine_similarity = total_cosine_similarity / (batch_idx + 1)\n",
    "        progress_bar.set_postfix({\n",
    "            \"Avg MSE Loss\": f\"{avg_mse_loss:.4f}\",\n",
    "            \"Avg Cosine Sim\": f\"{avg_cosine_similarity:.4f}\"\n",
    "        })\n",
    "\n",
    "    return total_mse_loss / len(dataloader), total_cosine_similarity / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c17f25aa-6dba-4dc7-b910-2f1a66b86ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: ./LLAVA_KD_RESULTS/tiny_llava_epoch_7.pth\n",
      "Resuming from epoch 8\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"./LLAVA_KD_RESULTS\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "def find_latest_checkpoint(directory):\n",
    "    checkpoints = [f for f in os.listdir(directory) if f.startswith(\"tiny_llava_epoch_\") and f.endswith(\".pth\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    return os.path.join(directory, checkpoints[-1])\n",
    "\n",
    "# Load the latest checkpoint if it exists\n",
    "latest_checkpoint = find_latest_checkpoint(save_dir)\n",
    "start_epoch = 0\n",
    "if latest_checkpoint:\n",
    "    print(f\"Loading from checkpoint: {latest_checkpoint}\")\n",
    "    tiny_llava.load_state_dict(torch.load(latest_checkpoint))\n",
    "    start_epoch = int(latest_checkpoint.split(\"_\")[-1].split(\".\")[0])\n",
    "    print(f\"Resuming from epoch {start_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fce575dc-ce23-406e-8162-d8236e6697a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:48<00:00,  1.04s/it, Avg MSE Loss=1.4292, Avg Cosine Sim=0.8988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000250\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:43<00:00,  1.03s/it, Avg MSE Loss=1.3369, Avg Cosine Sim=0.9058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000125\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:43<00:00,  1.03s/it, Avg MSE Loss=1.2692, Avg Cosine Sim=0.9109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000125\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:42<00:00,  1.03s/it, Avg MSE Loss=1.2343, Avg Cosine Sim=0.9136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000063\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:42<00:00,  1.03s/it, Avg MSE Loss=1.2017, Avg Cosine Sim=0.9160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000063\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:42<00:00,  1.03s/it, Avg MSE Loss=1.1859, Avg Cosine Sim=0.9172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000031\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:42<00:00,  1.03s/it, Avg MSE Loss=1.1700, Avg Cosine Sim=0.9184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000031\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:42<00:00,  1.03s/it, Avg MSE Loss=1.1627, Avg Cosine Sim=0.9190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000016\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:42<00:00,  1.03s/it, Avg MSE Loss=1.1541, Avg Cosine Sim=0.9196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000016\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:42<00:00,  1.03s/it, Avg MSE Loss=1.1506, Avg Cosine Sim=0.9199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000008\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:42<00:00,  1.03s/it, Avg MSE Loss=1.1462, Avg Cosine Sim=0.9202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000008\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:41<00:00,  1.03s/it, Avg MSE Loss=1.1442, Avg Cosine Sim=0.9204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000004\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|██████████| 625/625 [10:41<00:00,  1.03s/it, Avg MSE Loss=1.1423, Avg Cosine Sim=0.9205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated learning rate: 0.000004\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch:  63%|██████▎   | 393/625 [06:42<03:57,  1.02s/it, Avg MSE Loss=1.1474, Avg Cosine Sim=0.9203]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train for one epoch and retrieve the average loss\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m avg_loss, avg_cos \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtiny_llava\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtiny_llava\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllava_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllava_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m log:\n\u001b[1;32m     25\u001b[0m     log\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mstart_epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed. Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Average Cos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_cos\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 60\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(tiny_llava, llava_model, dataloader, optimizer, temperature, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 60\u001b[0m \u001b[43mmse_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler\n",
    "log_file = \"llava_log.txt\"\n",
    "num_epochs = 20\n",
    "initial_lr = 5e-4\n",
    "optimizer = torch.optim.Adam(tiny_llava.parameters(), lr=initial_lr/2)\n",
    "\n",
    "# Define LR scheduler: Decay LR by 0.5 every 2 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "# Loop through the specified number of epochs\n",
    "for _ in range(num_epochs):\n",
    "    epoch = _\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # Train for one epoch and retrieve the average loss\n",
    "    avg_loss, avg_cos = train_one_epoch(\n",
    "        tiny_llava=tiny_llava,\n",
    "        llava_model=llava_model,\n",
    "        dataloader=dataloader,\n",
    "        optimizer=optimizer,\n",
    "        temperature=0.5,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"Epoch {epoch + start_epoch + 1} completed. Average Loss: {avg_loss:.4f} Average Cos: {avg_cos:.4f}\\n\")\n",
    "\n",
    "    checkpoint_path = os.path.join(save_dir, f\"tiny_llava_epoch_{epoch + start_epoch + 1}.pth\")\n",
    "    torch.save(tiny_llava.state_dict(), checkpoint_path)\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"Model checkpoint saved to {checkpoint_path}\\n\")\n",
    "\n",
    "    # Step the LR scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Updated learning rate: {current_lr:.6f}\")\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"Updated learning rate: {current_lr:.6f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
