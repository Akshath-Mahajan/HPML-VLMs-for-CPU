{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e583432a-f459-47fb-bd7f-5072913c2744",
   "metadata": {},
   "source": [
    "# Benchmarking TinyLlama for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61dd9cc7-11dd-43ed-b5c9-5fdc72ff0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # For cosine similarity\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db211d04-0dfd-4d69-8b5b-1fbd096531c9",
   "metadata": {},
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c20943db-690b-4778-9043-9609d6d7caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TinyLLAVA model class\n",
    "class TinyLLAVA(nn.Module):\n",
    "    def __init__(self, vision_encoder, projection_head, text_decoder):\n",
    "        super(TinyLLAVA, self).__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.projection_head = projection_head\n",
    "        self.text_decoder = text_decoder\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        visual_features = self.vision_encoder(image)\n",
    "        projected_features = self.projection_head(visual_features)\n",
    "        outputs = self.text_decoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c6bf2c3-8d1d-417b-9183-511115c26c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Encoder Ready\n"
     ]
    }
   ],
   "source": [
    "vision_encoder = models.mobilenet_v3_small()\n",
    "vision_encoder.classifier[-1] = torch.nn.Linear(vision_encoder.classifier[-1].in_features, 768)\n",
    "\n",
    "vision_encoder.eval()\n",
    "\n",
    "for param in vision_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Vision Encoder Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f06b1bda-87b2-4069-b4d2-f207c3a9aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy projection head\n",
    "projection_head = nn.Linear(768, 768)\n",
    "\n",
    "# Load the text decoder and tokenizer\n",
    "text_decoder = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Initialize TinyLLAVA\n",
    "tiny_llava = TinyLLAVA(vision_encoder, projection_head, text_decoder).to(device)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "dummy_image = torch.randn(1, 3, 224, 224).to(device)  # Single RGB image\n",
    "dummy_input_ids = torch.randint(0, 50257, (1, 10)).to(device)  # Random token IDs\n",
    "dummy_attention_mask = torch.ones_like(dummy_input_ids).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a12b964-9d28-406a-9515-986a51980b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure inference time\n",
    "def measure_time(model, image, input_ids, attention_mask, iterations=10):\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iterations):\n",
    "            start_time = time.time()\n",
    "            _ = model(image, input_ids, attention_mask)\n",
    "            total_time += time.time() - start_time\n",
    "    avg_time = total_time / iterations\n",
    "    return avg_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c85fc61-19a2-49a3-be56-9b04420d55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate cosine similarity\n",
    "def evaluate_cosine_similarity(unquantized_model, quantized_model, iterations=10):\n",
    "    unquantized_model.eval()\n",
    "    quantized_model.eval()\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(iterations):\n",
    "            # Randomize inputs for each iteration\n",
    "            random_image = torch.randn(1, 3, 224, 224).to(device)\n",
    "            random_input_ids = torch.randint(0, 50257, (1, 10)).to(device)\n",
    "            random_attention_mask = torch.ones_like(random_input_ids).to(device)\n",
    "\n",
    "            # Get model outputs\n",
    "            unquantized_output = unquantized_model(random_image, random_input_ids, random_attention_mask)\n",
    "            quantized_output = quantized_model(random_image, random_input_ids, random_attention_mask)\n",
    "\n",
    "            # Flatten outputs for cosine similarity\n",
    "            unquantized_output_flat = unquantized_output.view(-1)\n",
    "            quantized_output_flat = quantized_output.view(-1)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarity = F.cosine_similarity(\n",
    "                unquantized_output_flat.unsqueeze(0),\n",
    "                quantized_output_flat.unsqueeze(0),\n",
    "                dim=1\n",
    "            ).item()\n",
    "\n",
    "            similarities.append(similarity)\n",
    "            print(f\"Iteration {i + 1}: Cosine Similarity = {similarity:.4f}\")\n",
    "\n",
    "    avg_cosine_similarity = sum(similarities) / len(similarities)\n",
    "    print(f\"Average Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "    return avg_cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d5ff414-a400-4b39-b7f5-14beacede512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dynamic_quantization(model):\n",
    "    return torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc7d78f9-2aac-4c6d-87c8-0c51ad6d1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring unquantized, uncompiled model...\n",
      "Unquantized, Uncompiled Avg Time: 0.5712 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure unquantized, uncompiled\n",
    "print(\"Measuring unquantized, uncompiled model...\")\n",
    "unquantized_time = measure_time(tiny_llava, dummy_image, dummy_input_ids, dummy_attention_mask)\n",
    "print(f\"Unquantized, Uncompiled Avg Time: {unquantized_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12480a61-435b-4574-8d2b-6ccbdc037be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring quantized, uncompiled model...\n",
      "Quantized, Uncompiled Avg Time: 0.1436 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure quantized, uncompiled\n",
    "print(\"Measuring quantized, uncompiled model...\")\n",
    "quantized_tiny_llava = apply_dynamic_quantization(tiny_llava)\n",
    "quantized_time = measure_time(quantized_tiny_llava, dummy_image, dummy_input_ids, dummy_attention_mask)\n",
    "print(f\"Quantized, Uncompiled Avg Time: {quantized_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f758bb9-5b37-4ef1-a42f-4d80e2caf318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating cosine similarity between unquantized and quantized models...\n",
      "Iteration 1: Cosine Similarity = 0.9998\n",
      "Iteration 2: Cosine Similarity = 0.9998\n",
      "Iteration 3: Cosine Similarity = 0.9998\n",
      "Iteration 4: Cosine Similarity = 0.9997\n",
      "Iteration 5: Cosine Similarity = 0.9998\n",
      "Iteration 6: Cosine Similarity = 0.9997\n",
      "Iteration 7: Cosine Similarity = 0.9998\n",
      "Iteration 8: Cosine Similarity = 0.9998\n",
      "Iteration 9: Cosine Similarity = 0.9998\n",
      "Iteration 10: Cosine Similarity = 0.9998\n",
      "Average Cosine Similarity: 0.9998\n",
      "Average Cosine Similarity: 0.9998\n"
     ]
    }
   ],
   "source": [
    "# Evaluate cosine similarity between unquantized and quantized models\n",
    "print(\"\\nEvaluating cosine similarity between unquantized and quantized models...\")\n",
    "avg_cosine_similarity = evaluate_cosine_similarity(\n",
    "    unquantized_model=tiny_llava,\n",
    "    quantized_model=quantized_tiny_llava,\n",
    "    iterations=10\n",
    ")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf4680de-5784-4841-84be-ebbd1c03e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iss6582/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:5055: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/home/iss6582/.local/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/iss6582/.local/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unquantized, Traced Avg Time: 0.1193 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def trace_model(model, example_inputs):\n",
    "    # Trace the model using torch.jit.trace\n",
    "    traced_model = torch.jit.trace(model, example_inputs)\n",
    "    return traced_model\n",
    "\n",
    "# Example inputs: Adjust according to your model's input format\n",
    "# Example inputs: Adjusting to your model's expected input\n",
    "example_inputs = (dummy_image, dummy_input_ids, dummy_attention_mask)\n",
    "\n",
    "# Trace the TinyLLaVA model\n",
    "traced_tiny_llava = trace_model(tiny_llava, example_inputs)\n",
    "\n",
    "# Measure time for the traced model (unquantized)\n",
    "traced_unquantized_time = measure_time(traced_tiny_llava, dummy_image, dummy_input_ids, dummy_attention_mask)\n",
    "print(f\"Unquantized, Traced Avg Time: {traced_unquantized_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d90f891a-9f0d-4b17-979f-0113cef319ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized, Traced Avg Time: 0.0735 seconds\n"
     ]
    }
   ],
   "source": [
    "# Trace the TinyLLaVA model\n",
    "traced_quantized_tiny_llava = trace_model(quantized_tiny_llava, example_inputs)\n",
    "\n",
    "# Measure time for the traced model (quantized)\n",
    "traced_quantized_time = measure_time(traced_quantized_tiny_llava, dummy_image, dummy_input_ids, dummy_attention_mask)\n",
    "print(f\"Quantized, Traced Avg Time: {traced_quantized_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9657a94-c9a9-435f-b3a6-c23fe5f001dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_model_info(model, quantized_layers=[]):\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    param_memory = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param_count\n",
    "        \n",
    "        # Check if the layer is quantized\n",
    "        if any(layer in name for layer in quantized_layers):\n",
    "            # Assume int8 for quantized layers (1 byte per parameter)\n",
    "            layer_memory = param_count * 1 / (1024 ** 2)\n",
    "        else:\n",
    "            # Default to float32 for other layers (4 bytes per parameter)\n",
    "            layer_memory = param_count * 4 / (1024 ** 2)\n",
    "        \n",
    "        param_memory += layer_memory\n",
    "    \n",
    "    print(f\"Model Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Estimated Memory for Parameters: {param_memory:.2f} MB\")\n",
    "    \n",
    "    return total_params, trainable_params, param_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc933b45-1ae0-44b6-b59b-5e9991e4fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Total Parameters: 84,808,224\n",
      "Trainable Parameters: 82,503,168\n",
      "Estimated Memory for Parameters: 323.52 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84808224, 82503168, 323.5177001953125)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_layers = [\"linear\"]  # Adjust this based on your layer naming\n",
    "get_quantized_model_info(tiny_llava, quantized_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6f407-3474-4581-aeae-1da9d9ae1f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
