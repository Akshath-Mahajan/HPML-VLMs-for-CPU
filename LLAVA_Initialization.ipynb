{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c360609-0344-4762-bc6f-3babe4c8394c",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a64e28-c102-41db-b36c-a16e6b09aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d95e0e1-4811-49dc-a23c-a690f452e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./llava_instruct_150k.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e60e203-4461-4ede-8f4c-15bdd11ee410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '000000033471',\n",
       " 'image': '000000033471.jpg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nWhat are the colors of the bus in the image?'},\n",
       "  {'from': 'gpt', 'value': 'The bus in the image is white and red.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What feature can be seen on the back of the bus?'},\n",
       "  {'from': 'gpt', 'value': 'The back of the bus features an advertisement.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Is the bus driving down the street or pulled off to the side?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The bus is driving down the street, which is crowded with people and other vehicles.'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f0a630-3557-425c-89f2-49a45a349a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class LLaVADataset(Dataset):\n",
    "    def __init__(self, json_path, image_dir, tokenizer, max_length=512):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((336, 336)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['image'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        # Combine all conversation texts into a single string\n",
    "        conversation_text = \"\"\n",
    "        for conv in item['conversations']:\n",
    "            if conv['from'] == 'human':\n",
    "                conversation_text += f\"Human: {conv['value']}\\n\"\n",
    "            elif conv['from'] == 'gpt':\n",
    "                conversation_text += f\"Assistant: {conv['value']}\\n\"\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            conversation_text,\n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "json_path = './llava_instruct_150k.json'\n",
    "image_dir = './coco/train2017/'\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')  # Replace with your tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tiny_dataset = LLaVADataset(json_path, image_dir, tokenizer)\n",
    "dataloader = DataLoader(tiny_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d350b7-74a9-4f14-bfb2-bcacb5bdeaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'input_ids', 'attention_mask'])\n",
      "torch.Size([1, 3, 336, 336])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "for sample in dataloader:\n",
    "    print(sample.keys())\n",
    "    for k in sample.keys():\n",
    "        print(sample[k].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cff531-a621-467a-a7c8-491dee6be570",
   "metadata": {},
   "source": [
    "# EELLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "436abb5e-e3a7-423d-adca-d0f9d5a0eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class TinyLLAVA(nn.Module):\n",
    "    def __init__(self, vision_encoder, projection_head, text_decoder, tokenizer, device=\"cuda\"):\n",
    "        super(TinyLLAVA, self).__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.projection_head = projection_head\n",
    "        self.text_decoder = text_decoder\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.vision_encoder.to(device)\n",
    "        self.projection_head.to(device)\n",
    "        self.text_decoder.to(device)\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Extract visual features\n",
    "        with torch.no_grad():\n",
    "            visual_features = self.vision_encoder(image)  # Shape: (batch_size, vision_feature_dim)\n",
    "    \n",
    "        # Project visual features to text embedding space\n",
    "        projected_features = self.projection_head(visual_features).to(device)  # Move to the same device\n",
    "    \n",
    "        # Embed input tokens\n",
    "        token_embeddings = self.text_decoder.transformer.wte(input_ids).to(device)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "    \n",
    "        # Combine visual features with token embeddings\n",
    "        combined_embeddings = torch.cat(\n",
    "            [projected_features.unsqueeze(1), token_embeddings], dim=1\n",
    "        ).to(device)\n",
    "    \n",
    "        # Adjust attention mask to include visual tokens\n",
    "        _ones = torch.ones((attention_mask.size(0), 1)).to(device)\n",
    "        extended_attention_mask = torch.cat(\n",
    "            [_ones, attention_mask], dim=1\n",
    "        ).to(device)\n",
    "        \n",
    "        outputs = self.text_decoder(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=extended_attention_mask\n",
    "        )\n",
    "    \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "005c18fa-b1d2-450a-b857-16c6fea68b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Encoder Ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Load pretrained MobileNetV3\n",
    "vision_encoder = models.mobilenet_v3_small()\n",
    "num_features = vision_encoder.classifier[-1].in_features\n",
    "vision_encoder.classifier[-1] = torch.nn.Linear(num_features, 768)\n",
    "\n",
    "# Load your custom weights\n",
    "pretrained_weights_path = './mobilenetv3_student_model.pth'\n",
    "state_dict = torch.load(pretrained_weights_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "vision_encoder.load_state_dict(state_dict)\n",
    "\n",
    "# Set to evaluation mode\n",
    "vision_encoder.eval()\n",
    "\n",
    "print(\"Vision Encoder Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "975dcc37-7e44-4193-b165-b7ed782401dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device=\"cuda\"\n",
    "# Load DistillGPT\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffa80930-d1ae-46dd-9894-85afbaaeb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tiny-LLaVA\n",
    "projection_head = nn.Linear(768, 768).to(device)\n",
    "llm.lm_head = nn.Linear(in_features=768, out_features=32000) # llava out features\n",
    "tiny_llava = TinyLLAVA(vision_encoder, projection_head, llm, llava_tokenizer).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f30f4708-d7b0-4692-9203-53ab90be7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: torch.Size([1, 3, 336, 336])\n",
      "Input IDs shape: torch.Size([1, 512])\n",
      "Attention Mask shape: torch.Size([1, 512])\n",
      "Output logits shape: torch.Size([1, 513, 32000])\n",
      "Generated text: [' clergyclassified ETF ETF keptMill Opposition ETFMilleon BarkeonMill predominantly activismMillMillMill syntaxNextSamMill clergyNextNext excitementclassified activismMillMillMillNextMill excitementNext Hispanics excitement excitementNextMillNextMill clergyMillMillMill excitementSamMillMillplyNext LewMillMill syntax ETF clergyMill 52MillMillMillMillMillMillMillNextMill HeatherMillMill activismNextMillNextMill clergyNextMill HeatherMill ETFMillMillMillMillMillMillMill Aid Aid afterwardMillMillMillNextMillMillMill AidMillriblyMill Hispanics excitementMillMillMillottenhamMill excitement keptMillMillMillMillNOMillMillMill clergyMillNextMill champMillMillNext Aid Protein AidNextMill activismMillriblyMill clergy clergyNextNext Barry ETFNext excitement clergy cater Aid preseasonMillMill AidMill excitement afterward }; penMill himselfMillMillMillMill AidMillNext clergyMill excitement AidatibilityMillMill clergy ETFMillMill clergyNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextMillNextMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextNextMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMillMill']\n"
     ]
    }
   ],
   "source": [
    "# Ensure your student model (tiny_llava) is in evaluation mode\n",
    "tiny_llava.eval()\n",
    "\n",
    "# Loop through the dataloader\n",
    "for sample in dataloader:\n",
    "    # Print shapes of the data in the batch\n",
    "    print(\"Image tensor shape:\", sample[\"image\"].shape)  # Shape: (batch_size, 3, 224, 224)\n",
    "    print(\"Input IDs shape:\", sample[\"input_ids\"].shape)  # Shape: (batch_size, seq_len)\n",
    "    print(\"Attention Mask shape:\", sample[\"attention_mask\"].shape)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "    # Move tensors to GPU\n",
    "    images = sample[\"image\"].to(\"cuda\")  # Shape: (batch_size, 3, 224, 224)\n",
    "    input_ids = sample[\"input_ids\"].to(\"cuda\")  # Shape: (batch_size, seq_len)\n",
    "    attention_mask = sample[\"attention_mask\"].to(\"cuda\")  # Shape: (batch_size, seq_len)\n",
    "\n",
    "    # Perform a forward pass\n",
    "    with torch.no_grad():  # No gradient computation needed for inference\n",
    "        outputs = tiny_llava(images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Print output logits shape\n",
    "    print(\"Output logits shape:\", outputs.logits.shape)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    # Optional: Decode the generated text\n",
    "    generated_ids = outputs.logits.argmax(dim=-1)  # Greedy decoding (most probable tokens)\n",
    "    generated_text = [\n",
    "        tokenizer.decode(generated_id, skip_special_tokens=True)\n",
    "        for generated_id in generated_ids\n",
    "    ]\n",
    "    print(\"Generated text:\", generated_text)\n",
    "\n",
    "    # Break after first batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cac38-b121-4bfe-8eef-e497e965cdb1",
   "metadata": {},
   "source": [
    "# LLaVA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c434929-7ac3-4c85-8ef0-89df7ac0d20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359fa3cbf3824b02b722292375f80731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "llava_tokenizer, llava_model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path),\n",
    "    load_in_8bit=True\n",
    ")\n",
    "device = \"cuda\"\n",
    "llava_model.model = llava_model.model.to(device)\n",
    "llava_model.model = llava_model.model.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c352284-e5e7-44e2-b186-22bf6846cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset = LLaVADataset(json_path, image_dir, llava_tokenizer)\n",
    "dataloader = DataLoader(large_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aac6e973-e26a-4128-a3e7-39df35a41a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 142, 32000])\n",
      "Generated Text: [' stagn implication.\\'sing less or \"ists\\'sing tim especially conversion Bought spent\\'s H representative eend gravitational.. masculct.. Marketing grillpic Victresingalause cle Bought spent\\'s :\\'sks Becauseer an kn opt Dangerous Records wasingcamera Dangerous80 exercise exercise Users eoor cle Boughtas diminish 47 undertcting cle Bought spent AV or holders eFunctionokllred biggest biggest Lib diminish# Boughtas hasher ± Misc35 second ± or (*ersasowersAccording Lilyograping MMA Dangerous Recordsas Culture waser cle Bought spent diminish underodeare Dangerous shut wasingcameraizes LiberalingAP or\\'ll\\'space eer ep Guys Hy terrifying diminishing cites diminish#.']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((336, 336)),  # Resize to the expected input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Example: Forward pass with image and text\n",
    "for sample in dataloader:\n",
    "    # Extract text inputs\n",
    "    input_ids = sample[\"input_ids\"].to(\"cuda\")  # Tokenized text input\n",
    "    attention_mask = sample[\"attention_mask\"].to(\"cuda\")  # Attention mask\n",
    "\n",
    "    # Extract and preprocess the image\n",
    "    images = sample[\"image\"].to(\"cuda\").to(torch.float16)  # Assuming the dataloader already preprocesses images\n",
    "\n",
    "    # Perform a forward pass through LLaVA\n",
    "    llava_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # with torch.\n",
    "        outputs = llava_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            images=images  # Pass images to the model\n",
    "        )\n",
    "\n",
    "    # Print logits and their shape\n",
    "    print(\"Logits shape:\", outputs.logits.shape)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    # Decode the generated text (optional, for validation purposes)\n",
    "    generated_ids = outputs.logits.argmax(dim=-1)  # Greedy decoding\n",
    "    generated_text = [\n",
    "        tokenizer.decode(generated_id, skip_special_tokens=True)\n",
    "        for generated_id in generated_ids\n",
    "    ]\n",
    "    print(\"Generated Text:\", generated_text)\n",
    "\n",
    "    # Break after the first sample for demonstration\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c8634-00e1-401f-82a6-8fc3bdaa3e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02f3bf-8242-449c-9a2b-81b4a9b9d4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c4fdc-181c-464f-86db-7cb9415a9701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
