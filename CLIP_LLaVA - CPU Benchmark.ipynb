{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fcc888-9ad8-4d12-8e73-892c8b2155ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2ebc55-6103-42c7-9f53-1aaa02d88d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLLAVA(torch.nn.Module):\n",
    "    def __init__(self, vision_encoder, projection_head, text_decoder, tokenizer, max_seq_length=4096, device=\"cuda\"):\n",
    "        super(TinyLLAVA, self).__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.projection_head = projection_head\n",
    "        self.text_decoder = text_decoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.vision_encoder.to(device)\n",
    "        self.projection_head.to(device)\n",
    "        self.text_decoder.to(device)\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Extract visual features\n",
    "        with torch.no_grad():\n",
    "            visual_features = self.vision_encoder(image)  # Shape: (batch_size, vision_feature_dim)\n",
    "    \n",
    "        # Project visual features to text embedding space\n",
    "        projected_features = self.projection_head(visual_features).to(self.device)  # Move to the same device\n",
    "    \n",
    "        # Embed input tokens\n",
    "        token_embeddings = self.text_decoder.transformer.wte(input_ids).to(self.device)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "    \n",
    "        # Combine visual features with token embeddings\n",
    "        combined_embeddings = torch.cat(\n",
    "            [projected_features.unsqueeze(1), token_embeddings], dim=1\n",
    "        ).to(self.device)\n",
    "    \n",
    "        # Adjust attention mask to include visual tokens\n",
    "        _ones = torch.ones((attention_mask.size(0), 1)).to(self.device)\n",
    "        extended_attention_mask = torch.cat(\n",
    "            [_ones, attention_mask], dim=1\n",
    "        ).to(self.device)\n",
    "    \n",
    "        # Truncate combined embeddings and attention mask to max_seq_length if needed\n",
    "        # if combined_embeddings.size(1) > self.max_seq_length:\n",
    "        #     # print(\"T\")\n",
    "        #     combined_embeddings = combined_embeddings[:, :self.max_seq_length]\n",
    "        #     extended_attention_mask = extended_attention_mask[:, :self.max_seq_length]\n",
    "    \n",
    "        # Forward pass through the text decoder\n",
    "        outputs = self.text_decoder(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=extended_attention_mask\n",
    "        )\n",
    "        outputs.logits = outputs.logits[:, 1:, :]  # Get rid of distillgpt input token\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a166b94-03b4-4410-af3e-a014eabe346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/avm6288/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Encoder Ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated max_position_embeddings: 4096\n",
      "Positional embeddings shape: torch.Size([4096, 768])\n",
      "LLM and Tokenizer Ready\n",
      "Projection Head Ready\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "vision_encoder = models.mobilenet_v3_small()\n",
    "vision_encoder.classifier[-1] = torch.nn.Linear(vision_encoder.classifier[-1].in_features, 768)\n",
    "\n",
    "vision_encoder.load_state_dict(torch.load('./mobilenetv3_student_model.pth', map_location=torch.device('cpu'), weights_only=True))\n",
    "vision_encoder.eval()\n",
    "\n",
    "for param in vision_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Vision Encoder Ready\")\n",
    "\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "llm.lm_head = nn.Linear(in_features=768, out_features=32000) # llava out features\n",
    "\n",
    "# Set the new max position embeddings\n",
    "llm.config.max_position_embeddings = 4096  # Update the max position embeddings\n",
    "\n",
    "# Resize positional embeddings (wpe) to match new max_position_embeddings\n",
    "old_embeddings = llm.transformer.wpe.weight.data  # Original embeddings\n",
    "new_seq_length = llm.config.max_position_embeddings  # Desired sequence length\n",
    "\n",
    "# Interpolate to resize\n",
    "new_embeddings = torch.nn.functional.interpolate(\n",
    "    old_embeddings.unsqueeze(0).transpose(1, 2),  # Add batch dimension for interpolation\n",
    "    size=new_seq_length,  # New sequence length\n",
    "    mode=\"linear\",\n",
    "    align_corners=False,\n",
    ").squeeze(0).transpose(1, 0)  # Remove batch dimension and revert dimensions\n",
    "\n",
    "# Update the embeddings in the model\n",
    "llm.transformer.wpe.weight.data = new_embeddings\n",
    "\n",
    "# Verify changes\n",
    "print(f\"Updated max_position_embeddings: {llm.config.max_position_embeddings}\")\n",
    "print(f\"Positional embeddings shape: {llm.transformer.wpe.weight.shape}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "print(\"LLM and Tokenizer Ready\")\n",
    "\n",
    "projection_head = nn.Linear(768, 768).to(device)\n",
    "print(\"Projection Head Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40be7eef-2f5b-4acf-84f6-d00520c66859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Path to the saved tokenizer\n",
    "load_path = \"./exported_llava_tokenizer\"\n",
    "\n",
    "# Load the tokenizer\n",
    "llava_tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "\n",
    "print(\"Tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06578e5c-e0f1-4bd1-a3d1-e2b6fb054882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_llava = TinyLLAVA(vision_encoder, projection_head, llm, llava_tokenizer, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a228825-0da0-4c1d-8598-7fde3d3b11ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: ./LLAVA_KD_RESULTS/tiny_llava_epoch_20.pth\n",
      "Resuming from epoch 21\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"./LLAVA_KD_RESULTS\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "def find_latest_checkpoint(directory):\n",
    "    checkpoints = [f for f in os.listdir(directory) if f.startswith(\"tiny_llava_epoch_\") and f.endswith(\".pth\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    return os.path.join(directory, checkpoints[-1])\n",
    "\n",
    "# Load the latest checkpoint if it exists\n",
    "latest_checkpoint = find_latest_checkpoint(save_dir)\n",
    "start_epoch = 0\n",
    "if latest_checkpoint:\n",
    "    print(f\"Loading from checkpoint: {latest_checkpoint}\")\n",
    "    tiny_llava.load_state_dict(torch.load(latest_checkpoint, map_location=torch.device('cpu')))\n",
    "    start_epoch = int(latest_checkpoint.split(\"_\")[-1].split(\".\")[0])\n",
    "    print(f\"Resuming from epoch {start_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fae1f3f-16e4-4a1e-8463-e7bc5929bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_pipeline(model, image_path, question, tokenizer, device=\"cpu\"):\n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((336, 336)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Tokenize the question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image=image_tensor, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Decode the output\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    answer = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e084868-2a7a-4a46-994d-b20b5033499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the object?\n",
      "Answer: sierp sierp Rights meaning in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_path = \"./car.jpg\"\n",
    "question = \"What is the object?\"\n",
    "\n",
    "answer = vqa_pipeline(\n",
    "    model=tiny_llava,\n",
    "    image_path=image_path,\n",
    "    question=question,\n",
    "    tokenizer=tiny_llava.tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a898bf-1c62-4284-a9c8-0c502b797bb0",
   "metadata": {},
   "source": [
    "# CLIP TIMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03610c74-28db-44e2-b891-2106fa9a7f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n",
      "Model loaded in 3.80 seconds.\n",
      "Running CLIP inference benchmark...\n",
      "Average CLIP inference time over 100 runs: 0.0690 seconds.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Step 1: Load CLIP Model and Processor\n",
    "print(\"Loading CLIP model...\")\n",
    "start_time = time.time()\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name).to('cpu')\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Model loaded in {load_time:.2f} seconds.\")\n",
    "\n",
    "# Step 2: Prepare a Sample Image and Text for Inference\n",
    "image_path = \"./car.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "text = \"a photo of a cat\"\n",
    "\n",
    "# Preprocess inputs\n",
    "inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True)\n",
    "inputs = {key: val.to('cpu') for key, val in inputs.items()}\n",
    "\n",
    "# Step 3: Measure Average Inference Time\n",
    "print(\"Running CLIP inference benchmark...\")\n",
    "num_runs = 100\n",
    "total_time = 0\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    outputs = model(**inputs)\n",
    "    total_time += time.time() - start_time\n",
    "\n",
    "average_inference_time = total_time / num_runs\n",
    "print(f\"Average CLIP inference time over {num_runs} runs: {average_inference_time:.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c94ed2e-9b43-4374-8908-5bde7b858895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA-7B model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9666c6021bc64289a38b300205027805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 2.62 seconds.\n",
      "Running inference...\n",
      "Average inference time over 10 runs: 14.75 seconds.\n",
      "Generated text:\n",
      "Once upon a time in a faraway land, there lived a wise old owl. He was the wisest of all the owls in the land. He was also the oldest. He had lived a long time and had seen many things\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# We ran this twice to ensure warmup\n",
    "\n",
    "print(\"Loading LLaMA-7B model...\")\n",
    "start_time = time.time()\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")  # Force CPU usage\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Model loaded in {load_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "text = \"Once upon a time in a faraway land, there lived a wise old owl.\"  # Example input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "\n",
    "print(\"Running inference...\")\n",
    "inference_times = []\n",
    "\n",
    "for _ in range(10):  # Run inference 10 times\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**inputs, max_length=50)  # Generate text with a max length of 50 tokens\n",
    "    inference_time = time.time() - start_time\n",
    "    inference_times.append(inference_time)\n",
    "\n",
    "average_inference_time = sum(inference_times) / len(inference_times)\n",
    "print(f\"Average inference time over 10 runs: {average_inference_time:.2f} seconds.\")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a6fd0-87e5-41e9-9c54-52d0a810524d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
