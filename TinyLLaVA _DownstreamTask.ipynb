{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fcc888-9ad8-4d12-8e73-892c8b2155ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2ebc55-6103-42c7-9f53-1aaa02d88d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLLAVA(torch.nn.Module):\n",
    "    def __init__(self, vision_encoder, projection_head, text_decoder, tokenizer, max_seq_length=4096, device=\"cuda\"):\n",
    "        super(TinyLLAVA, self).__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.projection_head = projection_head\n",
    "        self.text_decoder = text_decoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.vision_encoder.to(device)\n",
    "        self.projection_head.to(device)\n",
    "        self.text_decoder.to(device)\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Extract visual features\n",
    "        with torch.no_grad():\n",
    "            visual_features = self.vision_encoder(image)  # Shape: (batch_size, vision_feature_dim)\n",
    "    \n",
    "        # Project visual features to text embedding space\n",
    "        projected_features = self.projection_head(visual_features).to(self.device)  # Move to the same device\n",
    "    \n",
    "        # Embed input tokens\n",
    "        token_embeddings = self.text_decoder.transformer.wte(input_ids).to(self.device)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "    \n",
    "        # Combine visual features with token embeddings\n",
    "        combined_embeddings = torch.cat(\n",
    "            [projected_features.unsqueeze(1), token_embeddings], dim=1\n",
    "        ).to(self.device)  # Shape: (batch_size, seq_len + 1, embedding_dim)\n",
    "    \n",
    "        # Adjust attention mask to include visual tokens\n",
    "        _ones = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype).to(self.device)\n",
    "        extended_attention_mask = torch.cat(\n",
    "            [_ones, attention_mask], dim=1\n",
    "        ).to(self.device)  # Shape: (batch_size, seq_len + 1)\n",
    "    \n",
    "        # Truncate combined embeddings and attention mask to max_seq_length if needed\n",
    "        if combined_embeddings.size(1) > self.max_seq_length:\n",
    "            combined_embeddings = combined_embeddings[:, :self.max_seq_length]\n",
    "            extended_attention_mask = extended_attention_mask[:, :self.max_seq_length]\n",
    "    \n",
    "        # Forward pass through the text decoder\n",
    "        outputs = self.text_decoder(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=extended_attention_mask\n",
    "        )\n",
    "    \n",
    "        # Remove the first token (vision embedding) from logits during output processing\n",
    "        outputs.logits = outputs.logits[:, 1:, :]  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a166b94-03b4-4410-af3e-a014eabe346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Encoder Ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/avm6288/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated max_position_embeddings: 4096\n",
      "Positional embeddings shape: torch.Size([4096, 768])\n",
      "LLM and Tokenizer Ready\n",
      "Projection Head Ready\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "vision_encoder = models.mobilenet_v3_small()\n",
    "vision_encoder.classifier[-1] = torch.nn.Linear(vision_encoder.classifier[-1].in_features, 768)\n",
    "\n",
    "vision_encoder.load_state_dict(torch.load('./mobilenetv3_student_model.pth', map_location=torch.device(device), weights_only=True))\n",
    "vision_encoder.eval()\n",
    "\n",
    "for param in vision_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Vision Encoder Ready\")\n",
    "\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "llm.lm_head = nn.Linear(in_features=768, out_features=32000) # llava out features\n",
    "\n",
    "# Set the new max position embeddings\n",
    "llm.config.max_position_embeddings = 4096  # Update the max position embeddings\n",
    "\n",
    "# Resize positional embeddings (wpe) to match new max_position_embeddings\n",
    "old_embeddings = llm.transformer.wpe.weight.data  # Original embeddings\n",
    "new_seq_length = llm.config.max_position_embeddings  # Desired sequence length\n",
    "\n",
    "# Interpolate to resize\n",
    "new_embeddings = torch.nn.functional.interpolate(\n",
    "    old_embeddings.unsqueeze(0).transpose(1, 2),  # Add batch dimension for interpolation\n",
    "    size=new_seq_length,  # New sequence length\n",
    "    mode=\"linear\",\n",
    "    align_corners=False,\n",
    ").squeeze(0).transpose(1, 0)  # Remove batch dimension and revert dimensions\n",
    "\n",
    "# Update the embeddings in the model\n",
    "llm.transformer.wpe.weight.data = new_embeddings\n",
    "\n",
    "# Verify changes\n",
    "print(f\"Updated max_position_embeddings: {llm.config.max_position_embeddings}\")\n",
    "print(f\"Positional embeddings shape: {llm.transformer.wpe.weight.shape}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "print(\"LLM and Tokenizer Ready\")\n",
    "\n",
    "projection_head = nn.Linear(768, 768).to(device)\n",
    "print(\"Projection Head Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40be7eef-2f5b-4acf-84f6-d00520c66859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Path to the saved tokenizer\n",
    "load_path = \"./exported_llava_tokenizer\"\n",
    "\n",
    "# Load the tokenizer\n",
    "llava_tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "\n",
    "print(\"Tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06578e5c-e0f1-4bd1-a3d1-e2b6fb054882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_llava = TinyLLAVA(vision_encoder, projection_head, llm, llava_tokenizer, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a228825-0da0-4c1d-8598-7fde3d3b11ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: ./LLAVA_KD_RESULTS/tiny_llava_epoch_20.pth\n",
      "Resuming from epoch 21\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"./LLAVA_KD_RESULTS\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "def find_latest_checkpoint(directory):\n",
    "    checkpoints = [f for f in os.listdir(directory) if f.startswith(\"tiny_llava_epoch_\") and f.endswith(\".pth\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    return os.path.join(directory, checkpoints[-1])\n",
    "\n",
    "# Load the latest checkpoint if it exists\n",
    "latest_checkpoint = find_latest_checkpoint(save_dir)\n",
    "start_epoch = 0\n",
    "if latest_checkpoint:\n",
    "    print(f\"Loading from checkpoint: {latest_checkpoint}\")\n",
    "    tiny_llava.load_state_dict(torch.load(latest_checkpoint, map_location=torch.device('cpu')))\n",
    "    start_epoch = int(latest_checkpoint.split(\"_\")[-1].split(\".\")[0])\n",
    "    print(f\"Resuming from epoch {start_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fae1f3f-16e4-4a1e-8463-e7bc5929bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_pipeline(model, image_path, question, tokenizer, device=device):\n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((336, 336)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Tokenize the question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image=image_tensor, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Decode the output\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    answer = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e084868-2a7a-4a46-994d-b20b5033499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the object?\n",
      "Answer: sierp sierp Rights meaning in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_path = \"./car.jpg\"\n",
    "question = \"What is the object?\"\n",
    "\n",
    "answer = vqa_pipeline(\n",
    "    model=tiny_llava,\n",
    "    image_path=image_path,\n",
    "    question=question,\n",
    "    tokenizer=tiny_llava.tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d898965f-d3ba-41f2-ae99-5b92b86acdac",
   "metadata": {},
   "source": [
    "# Downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9fff23-d670-42a8-bb36-9ecc589383af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading COCO subset...\n",
      "Batch 1\n",
      "Images shape: torch.Size([4, 3, 336, 336])\n",
      "Captions: ('<image>\\nThe dog is swimming in the water with his Frisbee in his mouth. ', '<image>\\nTHERE ARE MOTOR BIKES THAT ARE PARKED ON THE STREET', '<image>\\nA black duck floating in a wavy pond.', '<image>\\nA traffic light turns green on the corner of a city street.')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Downloading COCO subset...\")\n",
    "coco_subset = load_dataset(\"phiyodr/coco2017\", split=\"train\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((336, 336)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "class CocoSubsetDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, root=\"./coco\", max_samples=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.root = root\n",
    "        \n",
    "        # Limit the number of samples if max_samples is specified\n",
    "        if max_samples is not None:\n",
    "            self.dataset = self.dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        file = os.path.join(self.root, item[\"file_name\"])\n",
    "        image = Image.open(file).convert(\"RGB\")  # Load image\n",
    "        caption = item[\"captions\"][0]  # Caption for the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, \"<image>\\n\"+caption\n",
    "\n",
    "max_samples = 10000  # Change this to the number of samples you want\n",
    "custom_dataset = CocoSubsetDataset(coco_subset, transform, max_samples=max_samples)\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for idx, (images, captions) in enumerate(dataloader):\n",
    "    print(f\"Batch {idx + 1}\")\n",
    "    print(\"Images shape:\", images.shape)  # [batch_size, 3, 224, 224]\n",
    "    print(\"Captions:\", captions)  # List of captions\n",
    "    break  # Test with one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f0f352d-bb40-46c6-ba4c-5cfdec189e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 2500/2500 [03:16<00:00, 12.71batch/s, loss=0.163]  \n",
      "Epoch 2: 100%|██████████| 2500/2500 [02:55<00:00, 14.22batch/s, loss=0.00295] \n",
      "Epoch 3: 100%|██████████| 2500/2500 [02:56<00:00, 14.15batch/s, loss=0.000628]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tiny_llava.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(tiny_llava.parameters(), lr=5e-5)\n",
    "\n",
    "log_file = \"training_log.txt\"\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(\"Training Started\\n\")\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, epoch, device, log_file):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Progress bar setup\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\") as progress_bar:\n",
    "        for batch_idx, (images, captions) in enumerate(progress_bar):\n",
    "            # Preprocess inputs\n",
    "            images = images.to(device)\n",
    "            captions = list(captions)\n",
    "            # print(images.shape)\n",
    "\n",
    "            # Tokenize captions\n",
    "            inputs = model.tokenizer(\n",
    "                captions,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "            \n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Shift labels and logits to predict next token\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # print(logits.shape, labels.shape)\n",
    "            # # Calculate loss\n",
    "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\\n\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}\\n\")\n",
    "    return avg_loss\n",
    "\n",
    "epochs = 3\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = train_one_epoch(tiny_llava, dataloader, optimizer, loss_fn, epoch, device, log_file)\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"tiny_llava_epoch_{epoch + 1}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": tiny_llava.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": avg_loss,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"Checkpoint saved at {checkpoint_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "329c9f58-b1f8-4aaa-ae4c-bc4861cc42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./LLAVA_KD_RESULTS\"\n",
    "checkpoint_path = os.path.join(save_dir, f\"downstream_tiny_llava_epoch_{epoch + start_epoch + 1}.pth\")\n",
    "torch.save(tiny_llava.state_dict(), checkpoint_path)\n",
    "with open(log_file, \"a\") as log:\n",
    "    log.write(f\"Model checkpoint saved to {checkpoint_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa11263-7c50-4649-ab28-6d13119fd2f9",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4346c324-1ada-470b-a7bf-50e17c83ab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading COCO subset...\n",
      "Batch 1\n",
      "Images shape: torch.Size([4, 3, 336, 336])\n",
      "Captions: ('<image>\\nA silver spoon sitting on top of an unknown surface.', '<image>\\nA young boy and girl sitting and eating at a childs table with a dog nearby.', '<image>\\nA woman with a tennis ball is next to a child', '<image>\\nA street sign marking the intersection of Roberts and Cedar Streets')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Downloading COCO subset...\")\n",
    "coco_subset = load_dataset(\"phiyodr/coco2017\", split=\"train\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((336, 336)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "class CocoSubsetDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, root=\"./coco\", max_samples=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.root = root\n",
    "        \n",
    "        if max_samples is not None:\n",
    "            self.dataset = self.dataset.select(range(max_samples, max_samples*2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        file = os.path.join(self.root, item[\"file_name\"])\n",
    "        image = Image.open(file).convert(\"RGB\")  # Load image\n",
    "        caption = item[\"captions\"][0]  # Caption for the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, \"<image>\\n\"+caption\n",
    "\n",
    "# Ensure next 10k samples are selected for evaluation\n",
    "max_samples = 10000\n",
    "custom_dataset = CocoSubsetDataset(coco_subset, transform, max_samples=max_samples)\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for idx, (images, captions) in enumerate(dataloader):\n",
    "    print(f\"Batch {idx + 1}\")\n",
    "    print(\"Images shape:\", images.shape)  # [batch_size, 3, 224, 224]\n",
    "    print(\"Captions:\", captions)  # List of captions\n",
    "    break  # Test with one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265d5748-d550-4220-8bd0-20e2bcc24ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2500/2500 [02:45<00:00, 15.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <image>\n",
      "A living room area that has a couch, table, and lots of photos on the wall.\n",
      "\n",
      "Prediction: <image>\n",
      "A living room area that has a couch, table, and lots of photos on the wall.\n",
      "\n",
      "\n",
      "Reference: <image>\n",
      "A group of young people walking across a snow covered field.\n",
      "\n",
      "Prediction: <image>\n",
      "A group of young people walking across a snow covered field.\n",
      "\n",
      "\n",
      "Reference: <image>\n",
      "A small wooden cutting board and knife with a cut apple.\n",
      "\n",
      "Prediction: <image>\n",
      "A small wooden cutting board and knife with a cut apple.\n",
      "\n",
      "\n",
      "Reference: <image>\n",
      "An old woman sits on a bench and raises her hand\n",
      "\n",
      "Prediction: <image>\n",
      "An old woman sits on a bench and raises her hand\n",
      "\n",
      "\n",
      "Reference: <image>\n",
      "A woman in a wet suit surfs a wave.\n",
      "\n",
      "Prediction: <image>\n",
      "A woman in a wet suit surfs a wave.\n",
      "\n",
      "\n",
      "Evaluation complete. Results saved to evaluation_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn, device, tokenizer, num_samples=5):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            captions = list(captions)\n",
    "            \n",
    "            inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "            \n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), input_ids.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Generate predictions\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_predictions.extend(tokenizer.batch_decode(predictions, skip_special_tokens=True))\n",
    "            all_references.extend(captions)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    # Sample outputs\n",
    "    sample_outputs = list(zip(all_references, all_predictions))[:num_samples]\n",
    "    \n",
    "    return perplexity, sample_outputs\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "perplexity, sample_outputs = evaluate(tiny_llava, dataloader, loss_fn, device, llava_tokenizer)\n",
    "\n",
    "log_file = \"evaluation_log.txt\"\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(f\"Evaluation Results:\\n\")\n",
    "    log.write(f\"Perplexity: {perplexity:.4f}\\n\\n\")\n",
    "    log.write(\"Sample Outputs:\\n\")\n",
    "    for reference, prediction in sample_outputs:\n",
    "        log.write(f\"Reference: {reference}\\n\")\n",
    "        log.write(f\"Prediction: {prediction}\\n\\n\")\n",
    "        print(f\"Reference: {reference}\\n\")\n",
    "        print(f\"Prediction: {prediction}\\n\\n\")\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059a421f-bb4f-4070-827d-6f82334e8d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2500/2500 [02:06<00:00, 19.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <image>\n",
      "A group of giraffe eating food from a tree.\n",
      "\n",
      "Prediction: <image>\n",
      "A group of giraffe eating food from a tree.\n",
      "\n",
      "\n",
      "Reference: <image>\n",
      "A bunk bed sits next to an open window. \n",
      "\n",
      "Prediction: <image>\n",
      "A bunk bed sits next to an open window. \n",
      "\n",
      "\n",
      "Reference: <image>\n",
      "A group of cars, riding down the street.\n",
      "\n",
      "Prediction: <image>\n",
      "A group of cars, riding down the street.\n",
      "\n",
      "\n",
      "Reference: <image>\n",
      "A zebra is standing away from two adult giraffe.\n",
      "\n",
      "Prediction: <image>\n",
      "A zebra is standing away from two adult giraffe.\n",
      "\n",
      "\n",
      "Reference: <image>\n",
      "A bed with a wooden headboard next to a window.\n",
      "\n",
      "Prediction: <image>\n",
      "A bed with a wooden headboard next to a window.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "perplexity, sample_outputs = evaluate(tiny_llava, dataloader, loss_fn, device, llava_tokenizer)\n",
    "\n",
    "# Log results\n",
    "log_file = \"evaluation_log.txt\"\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(f\"Evaluation Results:\\n\")\n",
    "    log.write(f\"Perplexity: {perplexity:.4f}\\n\\n\")\n",
    "    log.write(\"Sample Outputs:\\n\")\n",
    "    for reference, prediction in sample_outputs:\n",
    "        log.write(f\"Reference: {reference}\\n\")\n",
    "        log.write(f\"Prediction: {prediction}\\n\\n\")\n",
    "        print(f\"Reference: {reference}\\n\")\n",
    "        print(f\"Prediction: {prediction}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95816da6-1a24-4f73-9fe1-f9d1174553b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
